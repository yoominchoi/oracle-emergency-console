{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de80bf5-d331-4c84-af3b-25aa72305793",
   "metadata": {},
   "source": [
    "# Implementation of AI using RAG, Vector Search, and OCI AI Inference Model\n",
    "## Summarizing a Shooter's Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd3a56-5cb3-450b-9968-8a1a8d8b9b00",
   "metadata": {},
   "source": [
    "### Task 1: Loading the text sources from a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e05dd-fc7e-4107-9431-9e03486d8aff",
   "metadata": {},
   "source": [
    "The function will open all .txt files in a specified folder, read their contents, and split the text using the \"========\" separator. It will then collect all resulting chunks into an array. This array will be stored in a dictionary, with the file names as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771b1f71-4b82-4d80-91ec-07001dfc2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loadShooterDesc(directory_path):\n",
    "   shooter_desc = {}\n",
    "   \n",
    "   for filename in os.listdir(directory_path):\n",
    "      if filename.endswith(\".txt\"):  # assuming shooter_desc are in .txt files\n",
    "         file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "         with open(file_path) as f:\n",
    "            raw_shooter_desc = f.read()\n",
    "\n",
    "         filename_without_ext = os.path.splitext(filename)[0]  # remove .txt extension\n",
    "         shooter_desc[filename_without_ext] = [text.strip() for text in raw_shooter_desc.split('=====')]\n",
    "\n",
    "   return shooter_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "770d3797-2cea-4aac-8a12-186bd4840cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shooter_desc': [\"These are descriptions of a shooter or shooters.\\nwearing sunglasses\\nSunglasses are on his head.\\nIt's a guy.\\nWearing black jacket\\nhas a gun\\nhe is wearing a blue cap and a mask\"]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shooter_desc = loadShooterDesc('.')\n",
    "shooter_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07d956-7e42-469d-a48d-c2ba61679263",
   "metadata": {},
   "source": [
    "The final step in preparing the source data is to organize the dictionary for easy ingestion into the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa9b1048-8cbd-4cb1-80e2-f1b95e573ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"shooter_desc | These are descriptions of a shooter or shooters.\\nwearing sunglasses\\nSunglasses are on his head.\\nIt's a guy.\\nWearing black jacket\\nhas a gun\\nhe is wearing a blue cap and a mask\",\n",
       "  'path': 'shooter_desc'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [{'text': filename + ' | ' + section, 'path': filename} for filename, sections in shooter_desc.items() for section in sections]\n",
    "\n",
    "# Sample the resulting data\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09d07b-d36f-4838-8b6e-e4c92d3aa54e",
   "metadata": {},
   "source": [
    "## Task 2: Loading the Shooter Description chunks into the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9615-4536-44a0-83a8-70ed7c32c21c",
   "metadata": {},
   "source": [
    "### Step 1: Create a database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75494f83-c1a6-40b2-80c5-97e089f4ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose any user name\n",
    "un = \"user1\"\n",
    "pw = \"<<password>>\"\n",
    "cs = \"localhost/FREEPDB1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd0d9b1-b7e4-4433-a02f-9965240b2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "connection = oracledb.connect(user=un, password=pw, dsn=cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce646e9-af47-44bd-8492-8ccf370e20c5",
   "metadata": {},
   "source": [
    "### Step 2: Create the shooter_desc table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94297b-c19f-4d74-9851-73e2cbce7087",
   "metadata": {},
   "source": [
    "Need a table inside our database to store the vectors and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11884707-a5f9-496c-b444-59579eb95eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'shooter_desc'\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Create the table\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id NUMBER PRIMARY KEY,\n",
    "            payload CLOB CHECK (payload IS JSON),\n",
    "            vector VECTOR\n",
    "        )\"\"\"\n",
    "    try:\n",
    "        cursor.execute(create_table_sql)\n",
    "    except oracledb.DatabaseError as e:\n",
    "        raise\n",
    "\n",
    "    connection.autocommit = True    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79edc0f-ab58-44dc-85ea-b0ad8b2ee3f9",
   "metadata": {},
   "source": [
    "### Step 3: Vectorize the text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad761d6-bca9-4423-8f3b-09c103c955e4",
   "metadata": {},
   "source": [
    "Need an encoder to handle the vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "091a2a63-5f2a-48fc-b8e5-62dd704b3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e34cd-c569-4caf-a64a-935f8101f4d5",
   "metadata": {},
   "source": [
    "Process each chunk stored in the docs dictionary by encoding the text content. This will result in a structured format containing all chunks, their context (the source file name in this case), and the vector representation for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "154e0e7a-5504-4be9-8f53-871ef6429fdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbd46d3bf0a4c9f87dc80dc777ce2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import array\n",
    "\n",
    "# Define a list to store the data\n",
    "data = [\n",
    "    {\"id\": idx, \"vector_source\": row['text'], \"payload\": row}\n",
    "    for idx, row in enumerate(docs)\n",
    "]\n",
    "\n",
    "# Collect all texts for batch encoding\n",
    "texts = [f\"{row['vector_source']}\" for row in data]\n",
    "\n",
    "# Encode all texts in a batch\n",
    "embeddings = encoder.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Assign the embeddings back to your data structure\n",
    "for row, embedding in zip(data, embeddings):\n",
    "    row['vector'] = array.array(\"f\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6986c-1bf8-4792-acf5-8bcf623a131e",
   "metadata": {},
   "source": [
    "### Step 4: Insert the chunks + vectors in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18bab-9bac-4540-90ec-9327e492d264",
   "metadata": {},
   "source": [
    "We utilized a cursor object from the established database connection to execute a command that truncates the specified table. This operation clears all existing rows, effectively resetting the table to an empty state and preparing it for new data insertion.\n",
    "\n",
    "Next, the code prepares a list of tuples containing the new data, with each tuple comprising an ID, a JSON-encoded payload, and a vector.\n",
    "\n",
    "The json.dumps function converts the payload into a JSON string format, ensuring complex data structures are properly serialized for database storage.\n",
    "\n",
    "We then use the \"cursor.executemany\" method to insert all prepared tuples into the table in a single batch operation. This method is highly efficient for bulk inserts, reducing the number of database transactions and enhancing performance. Finally, the connection.commit method is called to commit the transaction, ensuring all changes are saved and made permanent in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ef2bbc2-704a-44e2-a75b-2e0b440c2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Truncate the table\n",
    "    cursor.execute(f\"truncate table {table_name}\")\n",
    "    prepared_data = [(row['id'], json.dumps(row['payload']), row['vector']) for row in data]\n",
    "\n",
    "    # Insert the data\n",
    "    cursor.executemany(\n",
    "        f\"\"\"INSERT INTO {table_name} (id, payload, vector)\n",
    "            VALUES (:1, :2, :3)\"\"\", prepared_data)\n",
    "    \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed457a84-78fe-4ade-af0c-0fd073009af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'text': \"shooter_desc | These are descriptions of a shooter or shooters.\\nwearing sunglasses\\nSunglasses are on his head.\\nIt's a guy.\\nWearing black jacket\\nhas a gun\\nhe is wearing a blue cap and a mask\", 'path': 'shooter_desc'}, array('f', [-0.017050394788384438, 0.05293922498822212, -0.04627383127808571, -0.04897970333695412, 0.11442756652832031, 0.024894798174500465, 0.17750421166419983, -0.04325743019580841, -0.09360206127166748, 0.1341097056865692, 0.0343773253262043, -0.046136274933815, 0.03531084209680557, -0.04318045452237129, 0.0699775293469429, -0.012814182788133621, -0.04406075179576874, 0.0021118817385286093, 0.03387780487537384, -0.07251087576150894, -0.04990013316273689, -0.058471620082855225, -0.0064957113936543465, -0.06550478935241699, -0.0465887114405632, -0.04710136353969574, 0.09361844509840012, -0.022491637617349625, -0.049941081553697586, 0.07235714793205261, 0.09345515817403793, -0.042309414595365524, -0.028224248439073563, -0.0257843267172575, -0.04262553155422211, -0.04996676370501518, -0.01626335084438324, 0.043805141001939774, -0.04616006091237068, -0.03223586082458496, -0.0632360503077507, 0.03331972286105156, -0.004776256158947945, -0.040358759462833405, 0.02035260759294033, 0.04051931947469711, -0.08164706081151962, 0.028874589130282402, -0.045641206204891205, 0.0018823909340426326, -0.06468832492828369, -0.058932412415742874, -0.03881452977657318, -0.07839379459619522, -0.017045019194483757, -0.08525237441062927, -0.03178803622722626, 0.025996897369623184, 0.03479849919676781, -0.03604529798030853, -0.03374176099896431, -0.06957581639289856, 0.011207791976630688, 0.05835360288619995, 0.07148250937461853, 0.06018739938735962, -0.03364754840731621, -0.03316214680671692, 0.05439657345414162, 0.026123737916350365, -0.02569652535021305, 0.0492769293487072, 0.02946452423930168, 0.03447938337922096, -0.09744683653116226, -0.09716157615184784, 0.016927149146795273, -0.07366406917572021, -0.014410209842026234, 0.05832141637802124, -0.046418894082307816, -0.06483194977045059, 0.02122577652335167, 0.0030058929696679115, -0.016564346849918365, -0.008836653083562851, 0.012891796417534351, -0.060589104890823364, -0.07587496936321259, 0.02689753845334053, -0.13041701912879944, 0.08769698441028595, 0.055140458047389984, -0.07189538329839706, 0.05936189740896225, 0.06856529414653778, 0.007369001861661673, 0.07747860997915268, -0.03780640289187431, 0.05687795206904411, 0.014790837652981281, -0.09484191238880157, 0.03122180700302124, -0.0365467369556427, 0.05899539962410927, 0.016625814139842987, -0.0074928984977304935, -0.05889510735869408, -0.005823924671858549, -0.00993599183857441, 0.01146286353468895, 0.10765483975410461, 0.004767358768731356, -0.07959446310997009, 0.021945204585790634, -0.10590751469135284, 0.015000100247561932, 0.0274151973426342, -0.06592103838920593, 0.022004060447216034, -0.004249596502631903, 0.09603356570005417, -0.05964116379618645, 0.030643895268440247, 0.020897209644317627, 0.0038757987786084414, 0.03421067073941231, 0.00716843456029892, 0.09761476516723633, 0.08455988019704819, -0.008765585720539093, 0.019424734637141228, 0.01683739200234413, -0.08308157324790955, -0.015577149577438831, -0.03379387408494949, -0.04809114709496498, 0.047592680901288986, 0.04653520509600639, -0.012063817121088505, -0.03666889667510986, 0.06221115216612816, 0.05040746182203293, 0.05445834994316101, -0.005107746925204992, 0.08457538485527039, -0.10181865841150284, -0.027325298637151718, 0.00747385248541832, -0.0015517447609454393, 0.038773003965616226, 0.007816655561327934, 0.0656605064868927, 0.10386497527360916, -0.07186747342348099, 0.02407497726380825, -0.03590226545929909, 0.05716673657298088, -0.02908153086900711, 0.004526596982032061, 0.03842420130968094, 0.020756077021360397, 0.06954453140497208, -0.06304221600294113, 0.025779234245419502, -0.022901520133018494, 0.006105157546699047, 0.060711685568094254, -0.02230420708656311, 0.0749274492263794, 0.02989601530134678, 0.015642976388335228, -0.07436761260032654, -0.0659923255443573, -0.07735361158847809, 0.0007017015595920384, -0.07963743805885315, -0.06268584728240967, 0.007288628723472357, -0.030204009264707565, 0.019484126940369606, -0.044785093516111374, 0.009552383795380592, 0.00035658356500789523, 0.026010526344180107, 0.0065610455349087715, -0.020187271758913994, 0.053605884313583374, 0.12338066846132278, -0.001395266386680305, -0.002385803498327732, 0.03165487200021744, 0.05416792258620262, -0.007036486640572548, 0.026319559663534164, 0.06507772952318192, 0.04074258357286453, -0.0007790946983732283, 0.018556730821728706, 0.08398257941007614, 0.04020906239748001, 0.038983073085546494, -0.028135409578680992, 0.06813226640224457, 0.01614733226597309, 0.04303324595093727, -0.010732683353126049, -0.029470965266227722, -0.03779099881649017, -0.05231311172246933, -0.029383808374404907, -0.0007651025662198663, -0.0766921266913414, -0.03031071275472641, 0.06211905926465988, -0.022348374128341675, -0.02272873744368553, 0.003155876649543643, 0.006148437969386578, 0.06321515887975693, -0.08878806978464127, 0.0627613291144371, 0.009283906780183315, 5.7805316737350005e-33, 0.02355419099330902, 0.007423540577292442, -0.01864774525165558, -0.0202350914478302, -0.01018702331930399, 0.024515999481081963, 0.0299624502658844, -0.051355719566345215, 0.10334137827157974, -0.056449007242918015, -0.023057272657752037, 0.06192939355969429, 0.029459303244948387, -0.024228012189269066, 0.01602480560541153, -0.08268433809280396, -0.08261717110872269, -0.026170674711465836, -0.0526561439037323, 0.07300334423780441, 0.038240380585193634, -0.09286686778068542, 0.03035224787890911, -0.04673861339688301, -0.0013680991251021624, 0.052828073501586914, 0.07929868251085281, -0.009895301423966885, -0.024795208126306534, 0.006755264010280371, 0.028473736718297005, -0.038538284599781036, -0.05141065642237663, 0.047576047480106354, -0.03503710404038429, 0.009277312085032463, -0.08787239342927933, -0.06320592015981674, 0.05853952839970589, -0.04083909094333649, -0.04767507687211037, -0.041230060160160065, -0.002390877576544881, 0.04545801505446434, 0.015509899705648422, -0.04407034441828728, -0.0009692671010270715, 0.04873407259583473, -0.06495095789432526, 0.017178531736135483, -0.06472188234329224, -0.008495749905705452, -0.041015416383743286, -0.03669595718383789, -0.0461859256029129, -0.03665125370025635, -0.1658683568239212, 0.013732172548770905, 0.0011283622588962317, 0.016302790492773056, 0.052888646721839905, -0.02273677848279476, -0.05453145503997803, 0.10157497227191925, -0.0001958162902155891, 0.021046873182058334, -0.0952119454741478, -0.030904408544301987, 0.010594021528959274, -0.00827305018901825, -0.0032965117134153843, -0.07577695697546005, 0.021071268245577812, 0.08185785263776779, -0.074942946434021, 0.011218586005270481, -0.049848902970552444, 0.05435202270746231, 0.10288026928901672, 0.044175129383802414, 0.04453592374920845, -0.01821444183588028, -0.0266911368817091, 0.0530320405960083, 0.006309058982878923, -0.005261826328933239, 0.03066081926226616, -0.014949098229408264, -0.022654667496681213, -0.054475367069244385, -0.05126548931002617, -0.004185102414339781, 0.03551102057099342, 0.04323677718639374, -0.03912697732448578, 5.582775908107465e-32, 0.005118174012750387, 0.05582926794886589, -0.020316394045948982, -0.015985075384378433, -0.030868835747241974, 0.08508720993995667, -0.01728149875998497, -0.0751512423157692, -0.011503410525619984, -0.05219465121626854, -0.0007648466853424907, -0.05266851931810379, 0.0524417906999588, -0.006985267158597708, -0.04788408428430557, -0.051156166940927505, -0.04727590084075928, 0.01086190715432167, 0.06461498886346817, -0.02792450226843357, 0.01244637556374073, 0.03741960600018501, 0.009965067729353905, 0.13708065450191498, 0.10018077492713928, 0.03369127959012985, 0.01162807084619999, 0.049852825701236725, 0.010899231769144535, 0.11602213233709335, -0.025330020114779472, 0.06307391077280045, 0.07879895716905594, -0.06652627140283585, 0.1017099991440773, 0.021089516580104828, 0.05080617591738701, 0.005335609894245863, 0.03136860206723213, 0.02526448294520378, -0.014108358882367611, -0.07037224620580673, -0.003038495546206832, -0.0008473035995848477, -0.047029752284288406, -0.013940824195742607, 0.05180075392127037, -0.015045328065752983, -0.07333965599536896, 0.019073225557804108, 0.03418119624257088, 0.003504642751067877, -0.015241050161421299, -0.008309697732329369, -0.010027331300079823, -0.021839652210474014, 0.03634116053581238, 0.004426413681358099, -0.037800099700689316, -0.0816866010427475, -0.004571925383061171, -0.02928847447037697, -0.04491153359413147, -0.020433621481060982]))\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    # Define the query to select all rows from a table\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Print the rows\n",
    "    for row in rows[:5]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd55fe-8633-42df-a995-06f08a027d4a",
   "metadata": {},
   "source": [
    "# Vector retrieval and Large Language Model generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212e55b-2319-4629-ab9c-a3e4cd2a9dec",
   "metadata": {},
   "source": [
    "To integrate the vector database (Oracle Database 23ai) and retrieve text chunks close to the \"question\" in vector space, we'll use these chunks to create an LLM prompt and ask the Oracle Generative AI Service for a well-worded response. This Retrieval-Augmented Generation (RAG) approach combines retrieval and generation methods to enhance natural language processing. The retriever finds relevant documents by embedding queries and documents into the same vector space, providing context for the generator. The generator, using models like LLaMA 2, integrates these documents to produce accurate and detailed responses, making RAG a versatile tool in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e0c79-db7b-4e9f-ba32-230b8efa11fa",
   "metadata": {},
   "source": [
    "## Task 1: Vectorize the \"question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21383962-762d-4dbf-8ce7-5198a703a6b5",
   "metadata": {},
   "source": [
    "We will take the user's question, convert it into a vector, and feed this vector to the database. The database will then retrieve similar vectors and their associated metadata stored within it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245dcd3-2ebb-4362-b61c-46f7a3f57326",
   "metadata": {},
   "source": [
    "### Step 1: Define the SQL script used to retrieve the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c7c50f-abb3-40ae-a936-66fa94f4079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topK represents the number of top results to retrieve\n",
    "topK = 3\n",
    "\n",
    "sql = f\"\"\"select payload, vector_distance(vector, :vector, COSINE) as score\n",
    "from {table_name}\n",
    "order by score\n",
    "fetch approx first {topK} rows only\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4057fd-d773-4b9d-bd62-061859ddfa15",
   "metadata": {},
   "source": [
    "### Step 2: Transforming the question into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cef48e0-3767-47bf-ad05-9d5754191df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Combine all the descriptions of the shooter into 2 sentences\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc070e-b4fd-4640-b07a-e1e1780c0204",
   "metadata": {},
   "source": [
    "### Step 3: Executing the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3479c884-9710-4bed-914d-1de0b70dd42c",
   "metadata": {},
   "source": [
    "Next, write the retrieval code. Employ the same encoder as in previous text chunks, generating a vector representation of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57441951-ac8e-4890-b78f-326c2b25c4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    embedding = list(encoder.encode(question))\n",
    "    vector = array.array(\"f\", embedding)\n",
    "    \n",
    "    results  = []\n",
    "    \n",
    "    for (info, score, ) in cursor.execute(sql, vector=vector):\n",
    "        text_content = info.read()\n",
    "        results.append((score, json.loads(text_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b02992-3982-4817-aa36-8e9f0355ca44",
   "metadata": {},
   "source": [
    "The SQL query is executed with the provided vector parameter, fetching relevant information from the database. For each result, the code retrieves the text content stored in JSON format and appends it to a list along with the calculated similarity score. This process iterates through all fetched results, accumulating them in the results list.\n",
    "\n",
    "If we print the results, we obtain a list showing the \"score\" of each hit, which represents the distance in vector space between the question and the text chunk, as well as the metadata JSON embedded in each chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e80046-1db2-4b33-8a9a-cfd910c1838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.4276947372566744,\n",
      "  {'text': 'shooter_desc | These are descriptions of a shooter or shooters.\\n'\n",
      "           'wearing sunglasses\\n'\n",
      "           'Sunglasses are on his head.\\n'\n",
      "           \"It's a guy.\\n\"\n",
      "           'Wearing black jacket\\n'\n",
      "           'has a gun\\n'\n",
      "           'he is wearing a blue cap and a mask',\n",
      "   'path': 'shooter_desc'})]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf86323-557e-493f-b1cc-c5cb67a96f05",
   "metadata": {},
   "source": [
    "## Task 2: Create the LLM prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668509d2-113f-496a-a5fd-8e904b9983cf",
   "metadata": {},
   "source": [
    "Before sending anything to the LLM, we must ensure that our prompt does not exceed the maximum context length of the model. For LLaMA 2, this limit is 4,096 tokens, which includes both the input tokens (the prompt) and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b75c5d4-cbbd-4323-a104-b4dbc6f69935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "import sys\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "tokenizer.model_max_length = sys.maxsize\n",
    "\n",
    "def truncate_string(string, max_tokens):\n",
    "    # Tokenize the text and count the tokens\n",
    "    tokens = tokenizer.encode(string, add_special_tokens=True) \n",
    "    # Truncate the tokens to a maximum length\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    # transform the tokens back to text\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19295efa-f962-4270-a562-f048eec2dbfb",
   "metadata": {},
   "source": [
    "This code utilizes the Hugging Face Transformers library to tokenize text using the LlamaTokenizerFast model. The tokenizer is initialized from the pre-trained \"hf-internal-testing/llama-tokenizer\" model, with its \"model_max_length\" attribute set to \"sys.maxsize\" to accommodate extremely large inputs without length constraints.\n",
    "\n",
    "The \"truncate_string\" function takes a string and a maximum token count as inputs. It tokenizes the input string, truncates the tokenized sequence to the specified maximum length, and then decodes the truncated tokens back into a string. This function effectively shortens the text to a specified token limit while preserving its readable format, which is useful for tasks requiring length constraints on input text.\n",
    "\n",
    "To ensure sufficient space for the rest of the prompt and the answer, we will truncate our chunks to 1000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19d6052f-61f5-4951-af04-a8121a6afe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform docs into a string array using the \"paylod\" key\n",
    "docs_as_one_string = \"\\n=========\\n\".join([doc[\"text\"] for doc in docs])\n",
    "docs_truncated = truncate_string(docs_as_one_string, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7396be9-d1d0-4c2f-9f64-15ac659fddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <s>[INST] <<SYS>>\n",
      "    You are a helpful assistant named Oracle chatbot. \n",
      "    USE ONLY the sources below and ABSOLUTELY IGNORE any previous knowledge.\n",
      "    Use Markdown if appropriate.\n",
      "    Assume the customer needs a clear description. Do not add additional details if it is not on the text file. Do not repeat the same information.\n",
      "    <</SYS>> [/INST]\n",
      "\n",
      "    [INST]\n",
      "    Respond to PRECISELY to this question: \"Combine all the descriptions of the shooter into 2 sentences.\",  USING ONLY the following information and IGNORING ANY PREVIOUS KNOWLEDGE.\n",
      "    Include code snippets and commands where necessary.\n",
      "    NEVER mention the sources, always respond as if you have that knowledge yourself. Do NOT provide warnings or disclaimers.\n",
      "    =====\n",
      "    Sources: <s> shooter_desc | These are descriptions of a shooter or shooters.\n",
      "wearing sunglasses\n",
      "Sunglasses are on his head.\n",
      "It's a guy.\n",
      "Wearing black jacket\n",
      "has a gun\n",
      "he is wearing a blue cap and a mask\n",
      "    =====\n",
      "    Answer (One paragraph, maximum 2 sentences, maximum 10 words, 90% spartan):\n",
      "    [/INST]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\\\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful assistant named Oracle chatbot. \n",
    "    USE ONLY the sources below and ABSOLUTELY IGNORE any previous knowledge.\n",
    "    Use Markdown if appropriate.\n",
    "    Assume the customer needs a clear description. Do not add additional details if it is not on the text file. Do not repeat the same information.\n",
    "    <</SYS>> [/INST]\n",
    "\n",
    "    [INST]\n",
    "    Respond to PRECISELY to this question: \"{question}.\",  USING ONLY the following information and IGNORING ANY PREVIOUS KNOWLEDGE.\n",
    "    Include code snippets and commands where necessary.\n",
    "    NEVER mention the sources, always respond as if you have that knowledge yourself. Do NOT provide warnings or disclaimers.\n",
    "    =====\n",
    "    Sources: {docs_truncated}\n",
    "    =====\n",
    "    Answer (One paragraph, maximum 2 sentences, maximum 10 words, 90% spartan):\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "print(prompt)  # Print the prompt to verify its formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc13038-040e-4672-a65b-7bb8743f29fd",
   "metadata": {},
   "source": [
    "## Task 3: Call the Generative AI Service LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b225852-5b20-4b0a-a3a5-c93eff3a20cd",
   "metadata": {},
   "source": [
    "### Initialize the OCI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5cb1ea7-d622-480f-b995-e7ab029af3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import logging\n",
    "from oci.generative_ai_inference.models import LlamaLlmInferenceRequest, GenerateTextDetails, OnDemandServingMode\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "compartment_id = '<<compartment id>>'\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)\n",
    "\n",
    "# Service endpoint\n",
    "endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "# GenAI client\n",
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c971e9-620f-42f3-a692-0e0afd560755",
   "metadata": {},
   "source": [
    "### Make the call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a53307-8a3c-4646-9465-d3f12ede83bc",
   "metadata": {},
   "source": [
    "This code uses Oracle Cloud Infrastructure (OCI) to generate text with the “meta.llama-2-70b-chat” model. It creates an inference request with parameters like the input prompt, maximum tokens, and settings for output randomness (temperature and top_p). The is_stream attribute is set to False.\n",
    "\n",
    "The code specifies the serving mode, model ID, and compartment ID for the text generation request. This setup directs OCI on which model to use and how to process the request.\n",
    "\n",
    "Finally, the request is sent to OCI’s Generative AI inference client. The client processes it, returns the generated text, which is then cleaned of any extra whitespace, and printed in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b74211-9c7d-42ca-9447-3942b9f89616",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_request = oci.generative_ai_inference.models.LlamaLlmInferenceRequest()\n",
    "\n",
    "generate_text_request.prompt = prompt\n",
    "generate_text_request.is_stream = False #SDK doesn't support streaming responses, feature is under development\n",
    "generate_text_request.max_tokens = 1500\n",
    "generate_text_request.temperature = 0.1\n",
    "generate_text_request.top_p = 0.7\n",
    "generate_text_request.frequency_penalty = 0.0\n",
    "\n",
    "generate_text_detail = oci.generative_ai_inference.models.GenerateTextDetails(\n",
    "    serving_mode=OnDemandServingMode(model_id=\"meta.llama-2-70b-chat\"),\n",
    "    compartment_id=<<compartment id>>,\n",
    "    inference_request=generate_text_request\n",
    "\n",
    ")\n",
    "generate_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"meta.llama-2-70b-chat\")\n",
    "generate_text_detail.compartment_id = compartment_id\n",
    "generate_text_detail.inference_request = generate_text_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a799372c-8ed9-4108-8329-c66f8c340303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shooter is a male wearing a black jacket, blue cap, and a mask, holding a gun. He has sunglasses on his head.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    generate_text_response = generative_ai_inference_client.generate_text(generate_text_detail)\n",
    "    response = generate_text_response.data.inference_response.choices[0].text\n",
    "    # logging.info(\"Full Response: %s\", response.strip())\n",
    "except Exception as e:\n",
    "    logging.error(\"Error occurred: %s\", str(e))\n",
    "    response = None\n",
    "    \n",
    "if response:\n",
    "    # response_length = len(response.strip())\n",
    "    # print(f\"Response Length: {response_length}\")\n",
    "    # print(\"Summary of the shooter:\")\n",
    "    print(response.strip())\n",
    "else:\n",
    "    print(\"No response received or response is empty.\")\n",
    "\n",
    "# print(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f90ab-6b8f-4a4a-957d-168379d62f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
